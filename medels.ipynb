{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "825486ea",
   "metadata": {},
   "source": [
    "# TCN 간단 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995280cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e11de",
   "metadata": {},
   "source": [
    "## train, val, test 구성\n",
    "-   데이터 분포\n",
    "    -   normal: 10세트 X 10개 = 100개\n",
    "    -   missing1\n",
    "        -   A누락: 2세트 X 10개\n",
    "        -   B누락: 2세트 X 10개\n",
    "        -   C누락: 2세트 X 10개 -> 총 6세트, 60개\n",
    "    -   missing2\n",
    "        -   A만: 2세트 X 10개\n",
    "        -   B만: 2세트 X 10개\n",
    "        -   C만: 2세트 X 10개 -> 총 6세트, 60개\n",
    "    -   idle: 6세트 X 10개 = 60개\n",
    "\n",
    "-   세트 기준으로 스플릿, 하지만 missing이 유형별로 너무 적기 때문에 k-fold cross-validation진행\n",
    "-   test는 따로 수집\n",
    "-   test 유형\n",
    "    -   3행동이 전부 있는 유형\n",
    "        -   빠르개 하는 버전\n",
    "        -   느리게 하는 버전\n",
    "        -   중간중간 멈칫/머뭇거림 있는 버전\n",
    "    -   2행동\n",
    "        -   뚜껑이 열려있는 상태로 진행\n",
    "        -   약이 들어있는 상태로 진행\n",
    "        -   뚜껑을 닫은 마무리가 없는 상태로 종료\n",
    "    -   1행동\n",
    "        -   뚜껑을 열기만 함\n",
    "        -   약을 넣기만 함\n",
    "        -   뚜껑을 닫기만 함\n",
    "    -   idle\n",
    "        -   손만 소소하게 음직이는데 작업은 안하는 경우\n",
    "\n",
    "-   test는 학습과 별계로 학습용 데이터에 참여하지 않은 사람, 약간 다른 카메라 세팅, 프레임 단위 라벨링을 진행하여 수집함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bff241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_type_and_number(fname: str):\n",
    "    \"\"\"\n",
    "    video_normal_023_lange.csv\n",
    "    video_missing1_A_007_lange.csv\n",
    "    → (type_str, number_int)\n",
    "    \"\"\"\n",
    "    # video_(타입)_(번호)_lange.csv\n",
    "    m = re.match(r\"video_(.+)_(\\d+)_lange\\.csv\", fname)\n",
    "    if not m:\n",
    "        return None, None\n",
    "    type_str = m.group(1)          # normal, missing1_A, idle ...\n",
    "    number   = int(m.group(2))     # 1, 2, ...\n",
    "    return type_str, number\n",
    "\n",
    "\n",
    "def load_all_data_with_sets(data_root: str):\n",
    "    \"\"\"\n",
    "    data_root: 'data'\n",
    "    구조:\n",
    "      data/out_csv/{normal,missing1,missing2,idle}\n",
    "      data/out_npz/{normal,missing1,missing2,idle}\n",
    "\n",
    "    return:\n",
    "      data_dict: sample_name -> {\"landmarks\", \"labels\"}\n",
    "      meta_dict: sample_name -> {\"type\", \"number\", \"set_idx\", \"set_id\"}\n",
    "    \"\"\"\n",
    "    csv_root = os.path.join(data_root, \"out_csv\")\n",
    "    npz_root = os.path.join(data_root, \"out_npz\")\n",
    "\n",
    "    subfolders = [\n",
    "        d for d in os.listdir(csv_root)\n",
    "        if os.path.isdir(os.path.join(csv_root, d))\n",
    "    ]\n",
    "\n",
    "    data_dict = {}\n",
    "    meta_dict = {}\n",
    "\n",
    "    for sub in sorted(subfolders):\n",
    "        csv_dir = os.path.join(csv_root, sub)\n",
    "        npz_dir = os.path.join(npz_root, sub)\n",
    "\n",
    "        csv_files = sorted([f for f in os.listdir(csv_dir) if f.endswith(\".csv\")])\n",
    "\n",
    "        for csv_file in csv_files:\n",
    "            type_str, number = extract_type_and_number(csv_file)\n",
    "            if type_str is None:\n",
    "                print(\"[WARN] 이름 패턴 안맞음:\", csv_file)\n",
    "                continue\n",
    "\n",
    "            csv_base = os.path.splitext(csv_file)[0]          # video_xxx_007_lange\n",
    "            core_name = csv_base.replace(\"_lange\", \"\")        # video_xxx_007\n",
    "            npz_base  = \"hands_\" + core_name                  # hands_video_xxx_007\n",
    "\n",
    "            csv_path = os.path.join(csv_dir, csv_file)\n",
    "            npz_path = os.path.join(npz_dir, npz_base + \".npz\")\n",
    "\n",
    "            if not os.path.exists(npz_path):\n",
    "                print(\"[WARN] npz 없음:\", npz_path)\n",
    "                continue\n",
    "\n",
    "            # CSV 로드\n",
    "            df = pd.read_csv(csv_path)\n",
    "            if \"Unnamed: 0\" in df.columns:\n",
    "                df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "            labels = df.to_numpy(dtype=np.float32)\n",
    "\n",
    "            # NPZ 로드\n",
    "            npz = np.load(npz_path)\n",
    "            if \"hand_kps\" not in npz.files:\n",
    "                print(f\"[WARN] 'hand_kps' 키 없음: {npz_path}, keys={npz.files}\")\n",
    "                continue\n",
    "            landmarks = npz[\"hand_kps\"].astype(np.float32)\n",
    "\n",
    "            if len(landmarks) != len(labels):\n",
    "                print(\"[WARN] 길이 불일치:\", csv_file)\n",
    "                continue\n",
    "\n",
    "            # 세트 정보 계산\n",
    "            set_idx = (number - 1) // 10 + 1          # 1~10 → set1, 11~20 → set2 ...\n",
    "            set_id  = f\"{type_str}_set{set_idx}\"      # normal_set1, missing1_A_set2 ...\n",
    "\n",
    "            sample_name = f\"{sub}/{core_name}\"        # 예: normal/video_normal_023\n",
    "\n",
    "            data_dict[sample_name] = {\n",
    "                \"landmarks\": landmarks,\n",
    "                \"labels\": labels,\n",
    "            }\n",
    "            meta_dict[sample_name] = {\n",
    "                \"type\": type_str,\n",
    "                \"number\": number,\n",
    "                \"set_idx\": set_idx,\n",
    "                \"set_id\": set_id,\n",
    "            }\n",
    "\n",
    "    print(f\"[INFO] 총 샘플 수: {len(data_dict)}\")\n",
    "    print(f\"[INFO] 세트 개수: {len(set(m['set_id'] for m in meta_dict.values()))}\")\n",
    "    return data_dict, meta_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d6bc81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_group_kfold_splits(meta_dict, n_folds=4, seed=42):\n",
    "    \"\"\"\n",
    "    meta_dict: sample_name -> { 'set_id': ..., ... }\n",
    "    n_folds: K-fold 개수\n",
    "    return:\n",
    "      folds: list of dict\n",
    "        [\n",
    "          {\n",
    "            \"train_keys\": [... sample_name ...],\n",
    "            \"val_keys\":   [... sample_name ...],\n",
    "          },\n",
    "          ...\n",
    "        ]\n",
    "    \"\"\"\n",
    "    # 1) 모든 세트 ID 수집\n",
    "    set_ids = sorted(set(m[\"set_id\"] for m in meta_dict.values()))\n",
    "    print(\"[INFO] unique set_ids:\", len(set_ids))\n",
    "\n",
    "    # 2) 셔플\n",
    "    rnd = random.Random(seed)\n",
    "    rnd.shuffle(set_ids)\n",
    "\n",
    "    # 3) 세트 단위로 folds 분할\n",
    "    folds_set_ids = [[] for _ in range(n_folds)]\n",
    "    for i, sid in enumerate(set_ids):\n",
    "        folds_set_ids[i % n_folds].append(sid)\n",
    "\n",
    "    # 4) 각 fold마다 train/val 샘플 리스트 생성\n",
    "    folds = []\n",
    "    for fold_idx in range(n_folds):\n",
    "        val_set_ids   = set(folds_set_ids[fold_idx])\n",
    "        train_set_ids = set(sid for sid in set_ids if sid not in val_set_ids)\n",
    "\n",
    "        train_keys = []\n",
    "        val_keys   = []\n",
    "        for sample_name, meta in meta_dict.items():\n",
    "            if meta[\"set_id\"] in train_set_ids:\n",
    "                train_keys.append(sample_name)\n",
    "            elif meta[\"set_id\"] in val_set_ids:\n",
    "                val_keys.append(sample_name)\n",
    "\n",
    "        folds.append({\n",
    "            \"train_keys\": train_keys,\n",
    "            \"val_keys\":   val_keys,\n",
    "            \"train_set_ids\": train_set_ids,\n",
    "            \"val_set_ids\":   val_set_ids,\n",
    "        })\n",
    "\n",
    "        print(f\"[FOLD {fold_idx}] train sets: {len(train_set_ids)}, \"\n",
    "              f\"val sets: {len(val_set_ids)}, \"\n",
    "              f\"train samples: {len(train_keys)}, \"\n",
    "              f\"val samples: {len(val_keys)}\")\n",
    "\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03ae536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 총 샘플 수: 280\n",
      "[INFO] 세트 개수: 28\n",
      "[INFO] unique set_ids: 28\n",
      "[FOLD 0] train sets: 21, val sets: 7, train samples: 210, val samples: 70\n",
      "[FOLD 1] train sets: 21, val sets: 7, train samples: 210, val samples: 70\n",
      "[FOLD 2] train sets: 21, val sets: 7, train samples: 210, val samples: 70\n",
      "[FOLD 3] train sets: 21, val sets: 7, train samples: 210, val samples: 70\n"
     ]
    }
   ],
   "source": [
    "# 1) 전체 데이터 로드\n",
    "data_root = \"data\"\n",
    "all_data_dict, meta_dict = load_all_data_with_sets(data_root)\n",
    "\n",
    "# 2) K-fold 세트 스플릿 생성\n",
    "folds = build_group_kfold_splits(meta_dict, n_folds=4, seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918efc2d",
   "metadata": {},
   "source": [
    "## TNC\n",
    "-   colap 으로 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0608ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_window_indices(n_frames: int, window: int = 15, step: int = 5):\n",
    "    \"\"\"\n",
    "    프레임 개수 n_frames에서 (start, end) 윈도우 인덱스 리스트 생성.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for start in range(0, n_frames - window + 1, step):\n",
    "        end = start + window\n",
    "        indices.append((start, end))\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30140d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkWindowDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 landmarks_dict: dict,\n",
    "                 labels_dict: dict,\n",
    "                 window: int = 15,\n",
    "                 step: int = 5):\n",
    "        \"\"\"\n",
    "        landmarks_dict: sample_name -> (N, D)\n",
    "        labels_dict   : sample_name -> (N, K)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.landmarks_dict = landmarks_dict\n",
    "        self.labels_dict    = labels_dict\n",
    "        self.window = window\n",
    "        self.step   = step\n",
    "\n",
    "        self.samples = sorted(landmarks_dict.keys())\n",
    "\n",
    "        # (sample_name, start, end) 리스트로 전체 윈도우를 전개\n",
    "        self.items = []\n",
    "        for sample in self.samples:\n",
    "            x = landmarks_dict[sample]\n",
    "            n_frames = x.shape[0]\n",
    "            win_idxs = build_window_indices(n_frames, window, step)\n",
    "            for (s, e) in win_idxs:\n",
    "                self.items.append((sample, s, e))\n",
    "\n",
    "        print(f\"[Dataset] samples: {self.samples}\")\n",
    "        print(f\"[Dataset] total windows: {len(self.items)}\")\n",
    "\n",
    "        # meta\n",
    "        any_sample = self.samples[0]\n",
    "        self.hand_dim    = landmarks_dict[any_sample].shape[1]\n",
    "        self.num_actions = labels_dict[any_sample].shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_name, start, end = self.items[idx]\n",
    "        x_all = self.landmarks_dict[sample_name]   # (N,D)\n",
    "        y_all = self.labels_dict[sample_name]      # (N,K)\n",
    "\n",
    "        x_win = x_all[start:end]                  # (T,D)\n",
    "        y_win = y_all[start:end]                  # (T,K)\n",
    "\n",
    "        x_t = torch.from_numpy(x_win).float()     # (T,D)\n",
    "        y_seq = torch.from_numpy(y_win).float()   # (T,K)\n",
    "        y_last = y_seq[-1]                        # (K,)\n",
    "\n",
    "        return {\n",
    "            \"x\": x_t,          # (T,D)\n",
    "            \"y_seq\": y_seq,    # (T,K)  (원하면 쓸 수 있도록 같이 반환)\n",
    "            \"y_last\": y_last,  # (K,)   (분류 타깃)\n",
    "            \"sample_name\": sample_name,\n",
    "            \"start\": start,\n",
    "            \"end\": end,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95993930",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"Causal conv를 위해 padding 뒤쪽을 잘라내는 모듈.\"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T_pad)\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size - 1) * dilation\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels,\n",
    "                               kernel_size, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels,\n",
    "                               kernel_size, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # residual connection (채널수가 바뀌면 1x1 conv로 맞춰줌)\n",
    "        self.downsample = (\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "            if in_channels != out_channels else None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.chomp1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.chomp2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TCNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes,\n",
    "                 channels=(32, 32), kernel_size=3, dropout=0.5):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_ch = input_dim\n",
    "        for i, out_ch in enumerate(channels):\n",
    "            dilation = 2 ** i\n",
    "            layers.append(\n",
    "                TemporalBlock(in_ch, out_ch,\n",
    "                              kernel_size=kernel_size,\n",
    "                              dilation=dilation,\n",
    "                              dropout=dropout)\n",
    "            )\n",
    "            in_ch = out_ch\n",
    "\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.fc = nn.Linear(in_ch, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, T, D)  # LandmarkWindowDataset에서 나오는 형태\n",
    "        return: (B, num_classes)\n",
    "        \"\"\"\n",
    "        # Conv1d: (B, C, T) 이므로 D ↔ C\n",
    "        x = x.transpose(1, 2)  # (B, D, T)\n",
    "        y = self.tcn(x)        # (B, C_out, T)\n",
    "        y_last = y[:, :, -1]   # 마지막 타임스텝만 사용 (B, C_out)\n",
    "        logits = self.fc(y_last)  # (B, num_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dfca9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[\"x\"].to(device)          # (B, T, D)\n",
    "        y = batch[\"y_last\"].to(device)     # (B, K) 0/1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)                  # (B, K)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        total_examples += x.size(0)\n",
    "\n",
    "        # 간단한 multi-label accuracy (threshold=0.5, 완전일치 비율)\n",
    "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "        correct = (preds == y).all(dim=1).sum().item()\n",
    "        total_correct += correct\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    acc = total_correct / total_examples\n",
    "    return avg_loss, acc\n",
    "\n",
    "\n",
    "def eval_one_epoch(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            y = batch[\"y_last\"].to(device)\n",
    "\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            total_examples += x.size(0)\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            correct = (preds == y).all(dim=1).sum().item()\n",
    "            total_correct += correct\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    acc = total_correct / total_examples\n",
    "    return avg_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efb92716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] samples: ['idle/video_idle_001', 'idle/video_idle_002', 'idle/video_idle_003', 'idle/video_idle_004', 'idle/video_idle_005', 'idle/video_idle_006', 'idle/video_idle_007', 'idle/video_idle_008', 'idle/video_idle_009', 'idle/video_idle_010', 'idle/video_idle_011', 'idle/video_idle_012', 'idle/video_idle_013', 'idle/video_idle_014', 'idle/video_idle_015', 'idle/video_idle_016', 'idle/video_idle_017', 'idle/video_idle_018', 'idle/video_idle_019', 'idle/video_idle_020', 'idle/video_idle_021', 'idle/video_idle_022', 'idle/video_idle_023', 'idle/video_idle_024', 'idle/video_idle_025', 'idle/video_idle_026', 'idle/video_idle_027', 'idle/video_idle_028', 'idle/video_idle_029', 'idle/video_idle_030', 'idle/video_idle_031', 'idle/video_idle_032', 'idle/video_idle_033', 'idle/video_idle_034', 'idle/video_idle_035', 'idle/video_idle_036', 'idle/video_idle_037', 'idle/video_idle_038', 'idle/video_idle_039', 'idle/video_idle_040', 'idle/video_idle_051', 'idle/video_idle_052', 'idle/video_idle_053', 'idle/video_idle_054', 'idle/video_idle_055', 'idle/video_idle_056', 'idle/video_idle_057', 'idle/video_idle_058', 'idle/video_idle_059', 'idle/video_idle_060', 'missing1/video_missing1_A_001', 'missing1/video_missing1_A_002', 'missing1/video_missing1_A_003', 'missing1/video_missing1_A_004', 'missing1/video_missing1_A_005', 'missing1/video_missing1_A_006', 'missing1/video_missing1_A_007', 'missing1/video_missing1_A_008', 'missing1/video_missing1_A_009', 'missing1/video_missing1_A_010', 'missing1/video_missing1_A_011', 'missing1/video_missing1_A_012', 'missing1/video_missing1_A_013', 'missing1/video_missing1_A_014', 'missing1/video_missing1_A_015', 'missing1/video_missing1_A_016', 'missing1/video_missing1_A_017', 'missing1/video_missing1_A_018', 'missing1/video_missing1_A_019', 'missing1/video_missing1_A_020', 'missing1/video_missing1_B_001', 'missing1/video_missing1_B_002', 'missing1/video_missing1_B_003', 'missing1/video_missing1_B_004', 'missing1/video_missing1_B_005', 'missing1/video_missing1_B_006', 'missing1/video_missing1_B_007', 'missing1/video_missing1_B_008', 'missing1/video_missing1_B_009', 'missing1/video_missing1_B_010', 'missing1/video_missing1_B_011', 'missing1/video_missing1_B_012', 'missing1/video_missing1_B_013', 'missing1/video_missing1_B_014', 'missing1/video_missing1_B_015', 'missing1/video_missing1_B_016', 'missing1/video_missing1_B_017', 'missing1/video_missing1_B_018', 'missing1/video_missing1_B_019', 'missing1/video_missing1_B_020', 'missing1/video_missing1_C_001', 'missing1/video_missing1_C_002', 'missing1/video_missing1_C_003', 'missing1/video_missing1_C_004', 'missing1/video_missing1_C_005', 'missing1/video_missing1_C_006', 'missing1/video_missing1_C_007', 'missing1/video_missing1_C_008', 'missing1/video_missing1_C_009', 'missing1/video_missing1_C_010', 'missing1/video_missing1_C_011', 'missing1/video_missing1_C_012', 'missing1/video_missing1_C_013', 'missing1/video_missing1_C_014', 'missing1/video_missing1_C_015', 'missing1/video_missing1_C_016', 'missing1/video_missing1_C_017', 'missing1/video_missing1_C_018', 'missing1/video_missing1_C_019', 'missing1/video_missing1_C_020', 'missing2/video_missing2_A_001', 'missing2/video_missing2_A_002', 'missing2/video_missing2_A_003', 'missing2/video_missing2_A_004', 'missing2/video_missing2_A_005', 'missing2/video_missing2_A_006', 'missing2/video_missing2_A_007', 'missing2/video_missing2_A_008', 'missing2/video_missing2_A_009', 'missing2/video_missing2_A_010', 'missing2/video_missing2_B_011', 'missing2/video_missing2_B_012', 'missing2/video_missing2_B_013', 'missing2/video_missing2_B_014', 'missing2/video_missing2_B_015', 'missing2/video_missing2_B_016', 'missing2/video_missing2_B_017', 'missing2/video_missing2_B_018', 'missing2/video_missing2_B_019', 'missing2/video_missing2_B_020', 'missing2/video_missing2_C_001', 'missing2/video_missing2_C_002', 'missing2/video_missing2_C_003', 'missing2/video_missing2_C_004', 'missing2/video_missing2_C_005', 'missing2/video_missing2_C_006', 'missing2/video_missing2_C_007', 'missing2/video_missing2_C_008', 'missing2/video_missing2_C_009', 'missing2/video_missing2_C_010', 'missing2/video_missing2_C_011', 'missing2/video_missing2_C_012', 'missing2/video_missing2_C_013', 'missing2/video_missing2_C_014', 'missing2/video_missing2_C_015', 'missing2/video_missing2_C_016', 'missing2/video_missing2_C_017', 'missing2/video_missing2_C_018', 'missing2/video_missing2_C_019', 'missing2/video_missing2_C_020', 'normal/video_normal_011', 'normal/video_normal_012', 'normal/video_normal_013', 'normal/video_normal_014', 'normal/video_normal_015', 'normal/video_normal_016', 'normal/video_normal_017', 'normal/video_normal_018', 'normal/video_normal_019', 'normal/video_normal_020', 'normal/video_normal_021', 'normal/video_normal_022', 'normal/video_normal_023', 'normal/video_normal_024', 'normal/video_normal_025', 'normal/video_normal_026', 'normal/video_normal_027', 'normal/video_normal_028', 'normal/video_normal_029', 'normal/video_normal_030', 'normal/video_normal_031', 'normal/video_normal_032', 'normal/video_normal_033', 'normal/video_normal_034', 'normal/video_normal_035', 'normal/video_normal_036', 'normal/video_normal_037', 'normal/video_normal_038', 'normal/video_normal_039', 'normal/video_normal_040', 'normal/video_normal_051', 'normal/video_normal_052', 'normal/video_normal_053', 'normal/video_normal_054', 'normal/video_normal_055', 'normal/video_normal_056', 'normal/video_normal_057', 'normal/video_normal_058', 'normal/video_normal_059', 'normal/video_normal_060', 'normal/video_normal_061', 'normal/video_normal_062', 'normal/video_normal_063', 'normal/video_normal_064', 'normal/video_normal_065', 'normal/video_normal_066', 'normal/video_normal_067', 'normal/video_normal_068', 'normal/video_normal_069', 'normal/video_normal_070', 'normal/video_normal_071', 'normal/video_normal_072', 'normal/video_normal_073', 'normal/video_normal_074', 'normal/video_normal_075', 'normal/video_normal_076', 'normal/video_normal_077', 'normal/video_normal_078', 'normal/video_normal_079', 'normal/video_normal_080']\n",
      "[Dataset] total windows: 9819\n",
      "[Dataset] samples: ['idle/video_idle_041', 'idle/video_idle_042', 'idle/video_idle_043', 'idle/video_idle_044', 'idle/video_idle_045', 'idle/video_idle_046', 'idle/video_idle_047', 'idle/video_idle_048', 'idle/video_idle_049', 'idle/video_idle_050', 'missing2/video_missing2_A_011', 'missing2/video_missing2_A_012', 'missing2/video_missing2_A_013', 'missing2/video_missing2_A_014', 'missing2/video_missing2_A_015', 'missing2/video_missing2_A_016', 'missing2/video_missing2_A_017', 'missing2/video_missing2_A_018', 'missing2/video_missing2_A_019', 'missing2/video_missing2_A_020', 'missing2/video_missing2_B_001', 'missing2/video_missing2_B_002', 'missing2/video_missing2_B_003', 'missing2/video_missing2_B_004', 'missing2/video_missing2_B_005', 'missing2/video_missing2_B_006', 'missing2/video_missing2_B_007', 'missing2/video_missing2_B_008', 'missing2/video_missing2_B_009', 'missing2/video_missing2_B_010', 'normal/video_normal_001', 'normal/video_normal_002', 'normal/video_normal_003', 'normal/video_normal_004', 'normal/video_normal_005', 'normal/video_normal_006', 'normal/video_normal_007', 'normal/video_normal_008', 'normal/video_normal_009', 'normal/video_normal_010', 'normal/video_normal_041', 'normal/video_normal_042', 'normal/video_normal_043', 'normal/video_normal_044', 'normal/video_normal_045', 'normal/video_normal_046', 'normal/video_normal_047', 'normal/video_normal_048', 'normal/video_normal_049', 'normal/video_normal_050', 'normal/video_normal_081', 'normal/video_normal_082', 'normal/video_normal_083', 'normal/video_normal_084', 'normal/video_normal_085', 'normal/video_normal_086', 'normal/video_normal_087', 'normal/video_normal_088', 'normal/video_normal_089', 'normal/video_normal_090', 'normal/video_normal_091', 'normal/video_normal_092', 'normal/video_normal_093', 'normal/video_normal_094', 'normal/video_normal_095', 'normal/video_normal_096', 'normal/video_normal_097', 'normal/video_normal_098', 'normal/video_normal_099', 'normal/video_normal_100']\n",
      "[Dataset] total windows: 3740\n",
      "FOLD 0\n",
      "train windows: 9819\n",
      "val windows  : 3740\n",
      "x: torch.Size([64, 15, 126])\n",
      "y_last: torch.Size([64, 3])\n"
     ]
    }
   ],
   "source": [
    "WINDOW = 15\n",
    "STEP   = 5\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def build_fold_dataloaders(fold_info, batch_size=64):\n",
    "    # landmarks_dict / labels_dict 구성\n",
    "    train_landmarks = {k: all_data_dict[k][\"landmarks\"] for k in fold_info[\"train_keys\"]}\n",
    "    train_labels    = {k: all_data_dict[k][\"labels\"]    for k in fold_info[\"train_keys\"]}\n",
    "    val_landmarks   = {k: all_data_dict[k][\"landmarks\"] for k in fold_info[\"val_keys\"]}\n",
    "    val_labels      = {k: all_data_dict[k][\"labels\"]    for k in fold_info[\"val_keys\"]}\n",
    "\n",
    "    train_dataset = LandmarkWindowDataset(\n",
    "        landmarks_dict=train_landmarks,\n",
    "        labels_dict=train_labels,\n",
    "        window=WINDOW,\n",
    "        step=STEP,\n",
    "    )\n",
    "    val_dataset = LandmarkWindowDataset(\n",
    "        landmarks_dict=val_landmarks,\n",
    "        labels_dict=val_labels,\n",
    "        window=WINDOW,\n",
    "        step=STEP,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_dataset, val_dataset, train_loader, val_loader\n",
    "\n",
    "# 예: FOLD 0 학습\n",
    "fold_idx = 0\n",
    "fold_info = folds[fold_idx]\n",
    "train_dataset, val_dataset, train_loader, val_loader = build_fold_dataloaders(fold_info)\n",
    "\n",
    "print(\"FOLD\", fold_idx)\n",
    "print(\"train windows:\", len(train_dataset))\n",
    "print(\"val windows  :\", len(val_dataset))\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"x:\", batch[\"x\"].shape)\n",
    "print(\"y_last:\", batch[\"y_last\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdd897ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== FOLD 0 ==========\n",
      "[Dataset] samples: ['idle/video_idle_001', 'idle/video_idle_002', 'idle/video_idle_003', 'idle/video_idle_004', 'idle/video_idle_005', 'idle/video_idle_006', 'idle/video_idle_007', 'idle/video_idle_008', 'idle/video_idle_009', 'idle/video_idle_010', 'idle/video_idle_011', 'idle/video_idle_012', 'idle/video_idle_013', 'idle/video_idle_014', 'idle/video_idle_015', 'idle/video_idle_016', 'idle/video_idle_017', 'idle/video_idle_018', 'idle/video_idle_019', 'idle/video_idle_020', 'idle/video_idle_021', 'idle/video_idle_022', 'idle/video_idle_023', 'idle/video_idle_024', 'idle/video_idle_025', 'idle/video_idle_026', 'idle/video_idle_027', 'idle/video_idle_028', 'idle/video_idle_029', 'idle/video_idle_030', 'idle/video_idle_031', 'idle/video_idle_032', 'idle/video_idle_033', 'idle/video_idle_034', 'idle/video_idle_035', 'idle/video_idle_036', 'idle/video_idle_037', 'idle/video_idle_038', 'idle/video_idle_039', 'idle/video_idle_040', 'idle/video_idle_051', 'idle/video_idle_052', 'idle/video_idle_053', 'idle/video_idle_054', 'idle/video_idle_055', 'idle/video_idle_056', 'idle/video_idle_057', 'idle/video_idle_058', 'idle/video_idle_059', 'idle/video_idle_060', 'missing1/video_missing1_A_001', 'missing1/video_missing1_A_002', 'missing1/video_missing1_A_003', 'missing1/video_missing1_A_004', 'missing1/video_missing1_A_005', 'missing1/video_missing1_A_006', 'missing1/video_missing1_A_007', 'missing1/video_missing1_A_008', 'missing1/video_missing1_A_009', 'missing1/video_missing1_A_010', 'missing1/video_missing1_A_011', 'missing1/video_missing1_A_012', 'missing1/video_missing1_A_013', 'missing1/video_missing1_A_014', 'missing1/video_missing1_A_015', 'missing1/video_missing1_A_016', 'missing1/video_missing1_A_017', 'missing1/video_missing1_A_018', 'missing1/video_missing1_A_019', 'missing1/video_missing1_A_020', 'missing1/video_missing1_B_001', 'missing1/video_missing1_B_002', 'missing1/video_missing1_B_003', 'missing1/video_missing1_B_004', 'missing1/video_missing1_B_005', 'missing1/video_missing1_B_006', 'missing1/video_missing1_B_007', 'missing1/video_missing1_B_008', 'missing1/video_missing1_B_009', 'missing1/video_missing1_B_010', 'missing1/video_missing1_B_011', 'missing1/video_missing1_B_012', 'missing1/video_missing1_B_013', 'missing1/video_missing1_B_014', 'missing1/video_missing1_B_015', 'missing1/video_missing1_B_016', 'missing1/video_missing1_B_017', 'missing1/video_missing1_B_018', 'missing1/video_missing1_B_019', 'missing1/video_missing1_B_020', 'missing1/video_missing1_C_001', 'missing1/video_missing1_C_002', 'missing1/video_missing1_C_003', 'missing1/video_missing1_C_004', 'missing1/video_missing1_C_005', 'missing1/video_missing1_C_006', 'missing1/video_missing1_C_007', 'missing1/video_missing1_C_008', 'missing1/video_missing1_C_009', 'missing1/video_missing1_C_010', 'missing1/video_missing1_C_011', 'missing1/video_missing1_C_012', 'missing1/video_missing1_C_013', 'missing1/video_missing1_C_014', 'missing1/video_missing1_C_015', 'missing1/video_missing1_C_016', 'missing1/video_missing1_C_017', 'missing1/video_missing1_C_018', 'missing1/video_missing1_C_019', 'missing1/video_missing1_C_020', 'missing2/video_missing2_A_001', 'missing2/video_missing2_A_002', 'missing2/video_missing2_A_003', 'missing2/video_missing2_A_004', 'missing2/video_missing2_A_005', 'missing2/video_missing2_A_006', 'missing2/video_missing2_A_007', 'missing2/video_missing2_A_008', 'missing2/video_missing2_A_009', 'missing2/video_missing2_A_010', 'missing2/video_missing2_B_011', 'missing2/video_missing2_B_012', 'missing2/video_missing2_B_013', 'missing2/video_missing2_B_014', 'missing2/video_missing2_B_015', 'missing2/video_missing2_B_016', 'missing2/video_missing2_B_017', 'missing2/video_missing2_B_018', 'missing2/video_missing2_B_019', 'missing2/video_missing2_B_020', 'missing2/video_missing2_C_001', 'missing2/video_missing2_C_002', 'missing2/video_missing2_C_003', 'missing2/video_missing2_C_004', 'missing2/video_missing2_C_005', 'missing2/video_missing2_C_006', 'missing2/video_missing2_C_007', 'missing2/video_missing2_C_008', 'missing2/video_missing2_C_009', 'missing2/video_missing2_C_010', 'missing2/video_missing2_C_011', 'missing2/video_missing2_C_012', 'missing2/video_missing2_C_013', 'missing2/video_missing2_C_014', 'missing2/video_missing2_C_015', 'missing2/video_missing2_C_016', 'missing2/video_missing2_C_017', 'missing2/video_missing2_C_018', 'missing2/video_missing2_C_019', 'missing2/video_missing2_C_020', 'normal/video_normal_011', 'normal/video_normal_012', 'normal/video_normal_013', 'normal/video_normal_014', 'normal/video_normal_015', 'normal/video_normal_016', 'normal/video_normal_017', 'normal/video_normal_018', 'normal/video_normal_019', 'normal/video_normal_020', 'normal/video_normal_021', 'normal/video_normal_022', 'normal/video_normal_023', 'normal/video_normal_024', 'normal/video_normal_025', 'normal/video_normal_026', 'normal/video_normal_027', 'normal/video_normal_028', 'normal/video_normal_029', 'normal/video_normal_030', 'normal/video_normal_031', 'normal/video_normal_032', 'normal/video_normal_033', 'normal/video_normal_034', 'normal/video_normal_035', 'normal/video_normal_036', 'normal/video_normal_037', 'normal/video_normal_038', 'normal/video_normal_039', 'normal/video_normal_040', 'normal/video_normal_051', 'normal/video_normal_052', 'normal/video_normal_053', 'normal/video_normal_054', 'normal/video_normal_055', 'normal/video_normal_056', 'normal/video_normal_057', 'normal/video_normal_058', 'normal/video_normal_059', 'normal/video_normal_060', 'normal/video_normal_061', 'normal/video_normal_062', 'normal/video_normal_063', 'normal/video_normal_064', 'normal/video_normal_065', 'normal/video_normal_066', 'normal/video_normal_067', 'normal/video_normal_068', 'normal/video_normal_069', 'normal/video_normal_070', 'normal/video_normal_071', 'normal/video_normal_072', 'normal/video_normal_073', 'normal/video_normal_074', 'normal/video_normal_075', 'normal/video_normal_076', 'normal/video_normal_077', 'normal/video_normal_078', 'normal/video_normal_079', 'normal/video_normal_080']\n",
      "[Dataset] total windows: 9819\n",
      "[Dataset] samples: ['idle/video_idle_041', 'idle/video_idle_042', 'idle/video_idle_043', 'idle/video_idle_044', 'idle/video_idle_045', 'idle/video_idle_046', 'idle/video_idle_047', 'idle/video_idle_048', 'idle/video_idle_049', 'idle/video_idle_050', 'missing2/video_missing2_A_011', 'missing2/video_missing2_A_012', 'missing2/video_missing2_A_013', 'missing2/video_missing2_A_014', 'missing2/video_missing2_A_015', 'missing2/video_missing2_A_016', 'missing2/video_missing2_A_017', 'missing2/video_missing2_A_018', 'missing2/video_missing2_A_019', 'missing2/video_missing2_A_020', 'missing2/video_missing2_B_001', 'missing2/video_missing2_B_002', 'missing2/video_missing2_B_003', 'missing2/video_missing2_B_004', 'missing2/video_missing2_B_005', 'missing2/video_missing2_B_006', 'missing2/video_missing2_B_007', 'missing2/video_missing2_B_008', 'missing2/video_missing2_B_009', 'missing2/video_missing2_B_010', 'normal/video_normal_001', 'normal/video_normal_002', 'normal/video_normal_003', 'normal/video_normal_004', 'normal/video_normal_005', 'normal/video_normal_006', 'normal/video_normal_007', 'normal/video_normal_008', 'normal/video_normal_009', 'normal/video_normal_010', 'normal/video_normal_041', 'normal/video_normal_042', 'normal/video_normal_043', 'normal/video_normal_044', 'normal/video_normal_045', 'normal/video_normal_046', 'normal/video_normal_047', 'normal/video_normal_048', 'normal/video_normal_049', 'normal/video_normal_050', 'normal/video_normal_081', 'normal/video_normal_082', 'normal/video_normal_083', 'normal/video_normal_084', 'normal/video_normal_085', 'normal/video_normal_086', 'normal/video_normal_087', 'normal/video_normal_088', 'normal/video_normal_089', 'normal/video_normal_090', 'normal/video_normal_091', 'normal/video_normal_092', 'normal/video_normal_093', 'normal/video_normal_094', 'normal/video_normal_095', 'normal/video_normal_096', 'normal/video_normal_097', 'normal/video_normal_098', 'normal/video_normal_099', 'normal/video_normal_100']\n",
      "[Dataset] total windows: 3740\n",
      "[fold 0] epoch 001 | train_loss=0.4874, val_loss=0.4924, train_acc=0.439, val_acc=0.350\n",
      "[fold 0] epoch 010 | train_loss=0.3410, val_loss=0.4374, train_acc=0.612, val_acc=0.409\n",
      "[fold 0] epoch 020 | train_loss=0.2952, val_loss=0.4184, train_acc=0.678, val_acc=0.466\n",
      "[fold 0] epoch 030 | train_loss=0.2679, val_loss=0.3643, train_acc=0.718, val_acc=0.541\n",
      "[fold 0] epoch 040 | train_loss=0.2586, val_loss=0.4138, train_acc=0.731, val_acc=0.514\n",
      "[fold 0] epoch 050 | train_loss=0.2544, val_loss=0.3714, train_acc=0.738, val_acc=0.552\n",
      "[fold 0] epoch 060 | train_loss=0.2440, val_loss=0.3759, train_acc=0.747, val_acc=0.555\n",
      "[fold 0] epoch 070 | train_loss=0.2423, val_loss=0.4252, train_acc=0.752, val_acc=0.498\n",
      "[fold 0] epoch 080 | train_loss=0.2273, val_loss=0.3587, train_acc=0.763, val_acc=0.561\n",
      "[fold 0] epoch 090 | train_loss=0.2230, val_loss=0.3410, train_acc=0.761, val_acc=0.589\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m best_val_acc  = \u001b[32m0.0\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m101\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     val_loss,   val_acc   = eval_one_epoch(model, val_loader, criterion)\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m val_loss < best_val_loss:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion)\u001b[39m\n\u001b[32m     14\u001b[39m logits = model(x)                  \u001b[38;5;66;03m# (B, K)\u001b[39;00m\n\u001b[32m     15\u001b[39m loss = criterion(logits, y)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m optimizer.step()\n\u001b[32m     20\u001b[39m total_loss += loss.item() * x.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\py312\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\py312\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\user\\miniconda3\\envs\\py312\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_fold_results = []\n",
    "\n",
    "for fold_idx, fold_info in enumerate(folds):\n",
    "    print(f\"\\n========== FOLD {fold_idx} ==========\")\n",
    "    train_dataset, val_dataset, train_loader, val_loader = build_fold_dataloaders(fold_info, batch_size=64)\n",
    "\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    input_dim = sample_batch[\"x\"].shape[-1]\n",
    "    num_classes = sample_batch[\"y_last\"].shape[-1]\n",
    "\n",
    "    model = TCNClassifier(input_dim, num_classes,\n",
    "                          channels=(32, 32),\n",
    "                          kernel_size=3,\n",
    "                          dropout=0.5).to(device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_acc  = 0.0\n",
    "\n",
    "    for epoch in range(1, 101):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "        val_loss,   val_acc   = eval_one_epoch(model, val_loader, criterion)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_val_acc  = val_acc\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1:\n",
    "            print(f\"[fold {fold_idx}] epoch {epoch:03d} | \"\n",
    "                  f\"train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n",
    "                  f\"train_acc={train_acc:.3f}, val_acc={val_acc:.3f}\")\n",
    "\n",
    "    print(f\"[fold {fold_idx}] BEST val_loss={best_val_loss:.4f}, val_acc={best_val_acc:.3f}\")\n",
    "    all_fold_results.append((best_val_loss, best_val_acc))\n",
    "\n",
    "print(\"\\n=== CV 결과 요약 ===\")\n",
    "for i, (vl, va) in enumerate(all_fold_results):\n",
    "    print(f\"fold {i}: val_loss={vl:.4f}, val_acc={va:.3f}\")\n",
    "print(\"평균 val_acc:\", sum(va for _, va in all_fold_results) / len(all_fold_results))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.12",
   "language": "python",
   "name": "py312_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
